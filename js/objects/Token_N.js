// Generated by CoffeeScript 1.12.4

/* ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
    * File: Token.coffee
    * -----------------
    * Contains code to create Tokens and to tokenize a string array
    +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 */

(function() {
  var tokenTypes;

  tokenTypes = {
    '^=': 'Equals',
    '^\\s+': 'White Space',
    '^[a-zA-Z]': 'Name',
    '^\\d+\\.?\\d*': 'Number',
    '^[()\\[\\]{},;`]': 'Special',
    '^(\\.|!{2}|\\*{1,2}|\\^{1,2}|/=?|\\+{1,2}|-|:|==|[<>]{1,2}=?|&&|\\|\\||\\$!?)': 'Symbol'
  };

  this.Token = (function() {
    function Token(body1, column, row1, type1, pres1, assoc1) {
      this.body = body1;
      this.column = column;
      this.row = row1;
      this.type = type1;
      this.pres = pres1;
      this.assoc = assoc1;

      /* ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
          * constructor (body, Int column, Int row, String type, Int pres, Int assoc)
          * ----------------
          * Constructor for a Token.
          * type :: [Integer, Special, White Space, Symbol, Name]
          ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
       */
    }

    return Token;

  })();

  this.tokenize = function(lineArr) {

    /* ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
        * tokenize ([String] lineArr)
        * ----------------
        * Takes a token and decided if its an Operand(true) or an Operator(false)
        ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
     */
    var _, assoc, body, col, dedents, i, ind, indLevels, j, k, len, len1, len2, match, pres, re, ref, ref1, ref2, res, row, s, type;
    indLevels = [0];
    res = [];
    for (i = 0, len = lineArr.length; i < len; i++) {
      ref = lineArr[i], row = ref[0], s = ref[1];
      col = 0;
      re = new RegExp('^\\s+');
      ind = re.test(s) ? (re.exec(s))[0].length : 0;
      s.replace(re, '');
      col = ind;
      if (ind > indLevels[indLevels.length - 1]) {
        res.push(new Token('', row, ind, 'Indent'));
        indLevels.push(ind);
      } else {
        if (ind < indLevels[indLevels.length - 1]) {
          dedents = indLevels.indexOf(ind);
          if (dedents === -1) {
            throw 'Inconsistent Indents';
          }
          ref1 = indLevels.length - dedents - 1;
          for (j = 0, len1 = ref1.length; j < len1; j++) {
            _ = ref1[j];
            res.push(new Token('', row, ind, 'Dedent'));
            indLevels.pop();
          }
        }
      }
      while (s !== '') {
        match = false;
        for (re in tokenTypes) {
          type = tokenTypes[re];
          re = new RegExp(re);
          if (re.test(s)) {
            body = (re.exec(s))[0];
            match = true;
            pres = getPres(body);
            assoc = getAssoc(body);
            res.push(new Token(body, col, row, type, pres, assoc));
            col += body.length;
            s = s.replace(re, '');
            break;
          }
        }
        if (!match) {
          throw 'Unknown character at (Row:' + row + ', Col:' + col;
        }
      }
      res.push(new Token('\n', col, row, 'Newline'));
    }
    if (indLevels.length > 1) {
      ref2 = indLevels.length - 1;
      for (k = 0, len2 = ref2.length; k < len2; k++) {
        _ = ref2[k];
        res.push(new Token('', 0, row + 1, 'Dedent'));
      }
    }
    return res;
  };

}).call(this);

//# sourceMappingURL=Token_N.js.map
